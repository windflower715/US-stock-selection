{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from copy import deepcopy\n",
    "from pyentrp import entropy as ent\n",
    "import warnings\n",
    "import datetime\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas_ta as ta\n",
    "import re \n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import  accuracy_score, classification_report, roc_auc_score, recall_score, precision_score,f1_score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "industrydic = pd.read_csv('us_stock_meta.csv')\n",
    "industrydic = dict(zip(industrydic.Ticker,industrydic.industry))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df =   pd.read_pickle('us_equity_data_final_spx.pkl')\n",
    "# df = df.loc['2018-01-01':]\n",
    "# with open('production_data_test.pkl','wb') as pkl :\n",
    "#     pickle.dump(df, pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_pickle('us_equity_data_final_spx.pkl')\n",
    "# newdf = pd.read_pickle('us_equity_data_updated.pickle')\n",
    "# df = df.append(newdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =   pd.read_pickle('production_data_test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('production_data_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>A UN Equity</th>\n",
       "      <th>A UN Equity.1</th>\n",
       "      <th>A UN Equity.2</th>\n",
       "      <th>A UN Equity.3</th>\n",
       "      <th>A UN Equity.4</th>\n",
       "      <th>A UN Equity.5</th>\n",
       "      <th>A UN Equity.6</th>\n",
       "      <th>A UN Equity.7</th>\n",
       "      <th>A UN Equity.8</th>\n",
       "      <th>...</th>\n",
       "      <th>ZTS UN Equity.15</th>\n",
       "      <th>ZTS UN Equity.16</th>\n",
       "      <th>ZTS UN Equity.17</th>\n",
       "      <th>ZTS UN Equity.18</th>\n",
       "      <th>ZTS UN Equity.19</th>\n",
       "      <th>ZTS UN Equity.20</th>\n",
       "      <th>ZTS UN Equity.21</th>\n",
       "      <th>ZTS UN Equity.22</th>\n",
       "      <th>ZTS UN Equity.23</th>\n",
       "      <th>ZTS UN Equity.24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>ASSET_TURNOVER</td>\n",
       "      <td>CASH_RATIO</td>\n",
       "      <td>EQY_SH_OUT</td>\n",
       "      <td>HIST_CALL_IMP_VOL</td>\n",
       "      <td>HIST_PUT_IMP_VOL</td>\n",
       "      <td>IS_EPS</td>\n",
       "      <td>MARKET_CAPITALIZATION_TO_BV</td>\n",
       "      <td>OPEN_INT_TOTAL_CALL</td>\n",
       "      <td>OPEN_INT_TOTAL_PUT</td>\n",
       "      <td>...</td>\n",
       "      <td>PX_LOW</td>\n",
       "      <td>PX_TO_BOOK_RATIO</td>\n",
       "      <td>PX_VOLUME</td>\n",
       "      <td>RETURN_COM_EQY</td>\n",
       "      <td>RETURN_COM_EQY_1</td>\n",
       "      <td>RETURN_ON_ASSET</td>\n",
       "      <td>RETURN_ON_INV_CAPITAL</td>\n",
       "      <td>SHORT_AND_LONG_TERM_DEBT</td>\n",
       "      <td>TOT_DEBT_TO_TOT_ASSET</td>\n",
       "      <td>TURNOVER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>323.018</td>\n",
       "      <td>21.408</td>\n",
       "      <td>21.408</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.52</td>\n",
       "      <td>9655.0</td>\n",
       "      <td>29580.0</td>\n",
       "      <td>...</td>\n",
       "      <td>71.47</td>\n",
       "      <td>19.7116</td>\n",
       "      <td>606012.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>43529900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>323.018</td>\n",
       "      <td>22.263</td>\n",
       "      <td>22.263</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.635</td>\n",
       "      <td>11946.0</td>\n",
       "      <td>49084.0</td>\n",
       "      <td>...</td>\n",
       "      <td>71.4</td>\n",
       "      <td>19.8023</td>\n",
       "      <td>615467.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>44315620.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>323.018</td>\n",
       "      <td>22.543000000000003</td>\n",
       "      <td>22.543000000000003</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.6002</td>\n",
       "      <td>13124.0</td>\n",
       "      <td>49437.0</td>\n",
       "      <td>...</td>\n",
       "      <td>72.26</td>\n",
       "      <td>19.9204</td>\n",
       "      <td>806599.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>58528120.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1340</th>\n",
       "      <td>2021-04-08 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>304.698</td>\n",
       "      <td>24.727</td>\n",
       "      <td>24.727</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.2891</td>\n",
       "      <td>8700</td>\n",
       "      <td>9755</td>\n",
       "      <td>...</td>\n",
       "      <td>158.64</td>\n",
       "      <td>20.178</td>\n",
       "      <td>574746</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.19226e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1341</th>\n",
       "      <td>2021-04-09 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>304.698</td>\n",
       "      <td>24.722</td>\n",
       "      <td>24.722</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.3665</td>\n",
       "      <td>8687</td>\n",
       "      <td>9835</td>\n",
       "      <td>...</td>\n",
       "      <td>158.47</td>\n",
       "      <td>20.1591</td>\n",
       "      <td>947109</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.51307e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1342</th>\n",
       "      <td>2021-04-12 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>304.698</td>\n",
       "      <td>25.114</td>\n",
       "      <td>25.114</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.3259</td>\n",
       "      <td>8694</td>\n",
       "      <td>9901</td>\n",
       "      <td>...</td>\n",
       "      <td>158.56</td>\n",
       "      <td>20.4945</td>\n",
       "      <td>751534</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.21912e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1343</th>\n",
       "      <td>2021-04-13 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>304.698</td>\n",
       "      <td>24.782</td>\n",
       "      <td>24.782</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.4337</td>\n",
       "      <td>8960</td>\n",
       "      <td>9941</td>\n",
       "      <td>...</td>\n",
       "      <td>161.68</td>\n",
       "      <td>20.6156</td>\n",
       "      <td>887748</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.45053e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1344</th>\n",
       "      <td>2021-04-14 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>304.698</td>\n",
       "      <td>25.8</td>\n",
       "      <td>25.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.3703</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>162.65</td>\n",
       "      <td>20.5664</td>\n",
       "      <td>414834</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.77911e+07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1345 rows Ã— 12514 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Unnamed: 0     A UN Equity A UN Equity.1 A UN Equity.2  \\\n",
       "0                     NaN  ASSET_TURNOVER    CASH_RATIO    EQY_SH_OUT   \n",
       "1              2018-01-01             NaN           NaN           NaN   \n",
       "2              2018-01-02             NaN           NaN       323.018   \n",
       "3              2018-01-03             NaN           NaN       323.018   \n",
       "4              2018-01-04             NaN           NaN       323.018   \n",
       "...                   ...             ...           ...           ...   \n",
       "1340  2021-04-08 00:00:00             NaN           NaN       304.698   \n",
       "1341  2021-04-09 00:00:00             NaN           NaN       304.698   \n",
       "1342  2021-04-12 00:00:00             NaN           NaN       304.698   \n",
       "1343  2021-04-13 00:00:00             NaN           NaN       304.698   \n",
       "1344  2021-04-14 00:00:00             NaN           NaN       304.698   \n",
       "\n",
       "           A UN Equity.3       A UN Equity.4 A UN Equity.5  \\\n",
       "0      HIST_CALL_IMP_VOL    HIST_PUT_IMP_VOL        IS_EPS   \n",
       "1                    NaN                 NaN           NaN   \n",
       "2                 21.408              21.408           NaN   \n",
       "3                 22.263              22.263           NaN   \n",
       "4     22.543000000000003  22.543000000000003           NaN   \n",
       "...                  ...                 ...           ...   \n",
       "1340              24.727              24.727           NaN   \n",
       "1341              24.722              24.722           NaN   \n",
       "1342              25.114              25.114           NaN   \n",
       "1343              24.782              24.782           NaN   \n",
       "1344                25.8                25.8           NaN   \n",
       "\n",
       "                    A UN Equity.6        A UN Equity.7       A UN Equity.8  \\\n",
       "0     MARKET_CAPITALIZATION_TO_BV  OPEN_INT_TOTAL_CALL  OPEN_INT_TOTAL_PUT   \n",
       "1                             NaN                  NaN                 NaN   \n",
       "2                            4.52               9655.0             29580.0   \n",
       "3                           4.635              11946.0             49084.0   \n",
       "4                          4.6002              13124.0             49437.0   \n",
       "...                           ...                  ...                 ...   \n",
       "1340                       8.2891                 8700                9755   \n",
       "1341                       8.3665                 8687                9835   \n",
       "1342                       8.3259                 8694                9901   \n",
       "1343                       8.4337                 8960                9941   \n",
       "1344                       8.3703                  NaN                 NaN   \n",
       "\n",
       "      ... ZTS UN Equity.15  ZTS UN Equity.16 ZTS UN Equity.17  \\\n",
       "0     ...           PX_LOW  PX_TO_BOOK_RATIO        PX_VOLUME   \n",
       "1     ...              NaN               NaN              NaN   \n",
       "2     ...            71.47           19.7116         606012.0   \n",
       "3     ...             71.4           19.8023         615467.0   \n",
       "4     ...            72.26           19.9204         806599.0   \n",
       "...   ...              ...               ...              ...   \n",
       "1340  ...           158.64            20.178           574746   \n",
       "1341  ...           158.47           20.1591           947109   \n",
       "1342  ...           158.56           20.4945           751534   \n",
       "1343  ...           161.68           20.6156           887748   \n",
       "1344  ...           162.65           20.5664           414834   \n",
       "\n",
       "     ZTS UN Equity.18  ZTS UN Equity.19 ZTS UN Equity.20  \\\n",
       "0      RETURN_COM_EQY  RETURN_COM_EQY_1  RETURN_ON_ASSET   \n",
       "1                 NaN               NaN              NaN   \n",
       "2                 NaN               NaN              NaN   \n",
       "3                 NaN               NaN              NaN   \n",
       "4                 NaN               NaN              NaN   \n",
       "...               ...               ...              ...   \n",
       "1340              NaN               NaN              NaN   \n",
       "1341              NaN               NaN              NaN   \n",
       "1342              NaN               NaN              NaN   \n",
       "1343              NaN               NaN              NaN   \n",
       "1344              NaN               NaN              NaN   \n",
       "\n",
       "           ZTS UN Equity.21          ZTS UN Equity.22       ZTS UN Equity.23  \\\n",
       "0     RETURN_ON_INV_CAPITAL  SHORT_AND_LONG_TERM_DEBT  TOT_DEBT_TO_TOT_ASSET   \n",
       "1                       NaN                       NaN                    NaN   \n",
       "2                       NaN                       NaN                    NaN   \n",
       "3                       NaN                       NaN                    NaN   \n",
       "4                       NaN                       NaN                    NaN   \n",
       "...                     ...                       ...                    ...   \n",
       "1340                    NaN                       NaN                    NaN   \n",
       "1341                    NaN                       NaN                    NaN   \n",
       "1342                    NaN                       NaN                    NaN   \n",
       "1343                    NaN                       NaN                    NaN   \n",
       "1344                    NaN                       NaN                    NaN   \n",
       "\n",
       "     ZTS UN Equity.24  \n",
       "0            TURNOVER  \n",
       "1                 NaN  \n",
       "2          43529900.0  \n",
       "3          44315620.0  \n",
       "4          58528120.0  \n",
       "...               ...  \n",
       "1340      9.19226e+07  \n",
       "1341      1.51307e+08  \n",
       "1342      1.21912e+08  \n",
       "1343      1.45053e+08  \n",
       "1344      6.77911e+07  \n",
       "\n",
       "[1345 rows x 12514 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('us_equity_data_final_spx.pkl', 'wb') as pkl:\n",
    "#     pickle.dump(df, pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class read_data(): \n",
    "    def __init__(self ): \n",
    "        self.df =   pd.read_pickle('production_data_test.pkl')\n",
    "        #self.df =   pd.read_pickle('dummydf.pickle')\n",
    "        self.df.sort_index(axis = 1, inplace = True)\n",
    "        self.df.fillna(method ='ffill', inplace = True)\n",
    "        self.df = self.df.apply(pd.to_numeric)\n",
    "        self.df.index = pd.to_datetime(self.df.index)\n",
    "        \n",
    "        self.meta = pd.read_csv('us_stock_meta.csv')\n",
    "        \n",
    "        self.daily_rts = self.df.loc[:,self.df.columns.get_level_values(1).isin(['PX_LAST'])]\n",
    "        self.daily_rts.columns = self.daily_rts.columns.droplevel(1)\n",
    "        \n",
    "        self.daily_rts = self.daily_rts/self.daily_rts.shift(1)  -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class feature_engineering(): \n",
    "    def __init__(self) : \n",
    "         read_data.__init__(self)\n",
    "            \n",
    "    def calculate_y_label(self, time_horizon,  threshold,join_original): \n",
    "        \"\"\"This function generates y label for classification model based on the time horizon and return threshold, \n",
    "        if the return for the next x days exceeds y percent, label it as 1\n",
    "        df: dataframe\n",
    "        time_horizon: int, time horizon for the window \n",
    "        threshold: return threshold to see if exceed\n",
    "        join_original: join with original dataframe\n",
    "        df = calculate_y_label(df, 15, 0.05, True) \"\"\"\n",
    "        price = self.df.loc[:, self.df.columns.get_level_values(1).isin(['PX_LAST'])]\n",
    "        rst = {} #pd.DataFrame(index = price.index, columns = price.columns )\n",
    "        for i in range(0, len(price)-time_horizon+1):\n",
    "            window = price.iloc[i: i+time_horizon]\n",
    "            window = window/window.iloc[0]-1 \n",
    "            window[window >= threshold] =1\n",
    "            window[window<= (-1) * threshold] = -1\n",
    "            window[(window<threshold) & (window> (-1) *threshold)] =0 \n",
    "            window[window == np.nan] = np.nan\n",
    "            date = price.index[i]\n",
    "            rst[date] = window.sum()\n",
    "        #     data ['y_direction'].loc[date] = window.sum()\n",
    "        rst_df = pd.concat(rst, axis = 1).transpose()\n",
    "\n",
    "        i = rst_df.columns.levels[1]  \n",
    "        j = ['y' +'_' + str(time_horizon)  + '_d' for x in i ]\n",
    "        rename_dict = dict(zip(i, j))\n",
    "        rst_df.rename(columns=rename_dict, level=1,inplace = True)\n",
    "\n",
    "        if join_original == True: \n",
    "            return  self.df.join(rst_df)\n",
    "        else:\n",
    "            return rst_df\n",
    "        return rstdf\n",
    "    \n",
    "    def calculate_structural_break (self, flds, join_original):\n",
    "        \"\"\"This function calculates x day changes for multiindex dataframe from pdblp\n",
    "        df:dataframe, input dataframe\n",
    "        flds:list, fields that you want to performe calculation on --[\"PX_LAST\", \"FUND_FLOW\",'VOLUME']\n",
    "        days:list,  x days of changes ,[5,15]\n",
    "        join_original: True or False, return calculated dataframe joined to the original df\n",
    "        eg: calculate_structural_break(df, ['PX_LAST','VOLUME','FUND_FLOW','PX_HIGH'], True)\"\"\"\n",
    "        #t,t1,tn = 252,251,242\n",
    "        t,t1,tn = 30,60,90\n",
    "        y = np.log( self.df.loc[:,self.df.columns.get_level_values(1).isin(flds)])\n",
    "        sigmasq = (( (y - y.shift(1))**2).rolling(t1).sum() )* (t-1)**(-1) \n",
    "        rst_df = (y-y.shift(3*26)) * (np.sqrt(sigmasq) * np.sqrt(tn)) **(-1)\n",
    "\n",
    "        i = rst_df.columns.levels[1]   # OP's suggestion, for more flexibility!\n",
    "        j = [x + '_sb' for x in i ]\n",
    "        rename_dict = dict(zip(i, j))\n",
    "        rst_df.rename(columns=rename_dict, level=1,inplace = True)\n",
    "\n",
    "        if join_original == True: \n",
    "            return self.df.join(rst_df)\n",
    "        else:\n",
    "            return rst_df   \n",
    "\n",
    "    def calculate_group_diff(self, flds ,tkr_group,join_original ): \n",
    "        \"\"\"This function calculates mean row difference by each group,\n",
    "        the purpose of this is to compare data against group performance\n",
    "        df: dataframe\n",
    "        flds: list, fields to perform calculation on\n",
    "        tkr_group:list,  the group to perform mean row difference,\n",
    "        join_original:boolean, true or false, join with original dataframe\n",
    "\n",
    "        calculate_group_diff(df, ['PX_LAST_5_dchg','VOLUME_5_dchg','FUND_FLOW_5_d_chg'],\n",
    "                         list(meta[meta['industry'] == 'Thematic']['Ticker'].unique()), True  )\"\"\"\n",
    "        rst = defaultdict()\n",
    "\n",
    "        for i_flds in flds: \n",
    "            subdf = self.df[tkr_group].loc[:, self.df[tkr_group].columns.get_level_values(1).isin([i_flds])]\n",
    "            rst[i_flds] = subdf.sub(subdf.median(axis = 1), axis=0)\n",
    "            i = subdf.columns.levels[1]   # OP's suggestion, for more flexibility!\n",
    "            j = [x +  '_ab' for x in i ]\n",
    "            rename_dict = dict(zip(i, j))\n",
    "            rst[i_flds].rename(columns=rename_dict, level=1,inplace = True)\n",
    "\n",
    "        rst_df = pd.concat(rst.values(),axis=1)\n",
    "        if join_original == True: \n",
    "            return self.df.join(rst_df)\n",
    "        else:\n",
    "            return rst_df\n",
    "        \n",
    "    def calculate_group_diff_agg(self,flds, agg_type,join_original ):\n",
    "        \"\"\" This function calculates aggregated difference by group with the calculate_group_diff function\n",
    "        df: dataframe, comparing all companies against group benchmark(mean)\n",
    "        flds: list, fields to perform calculation on \n",
    "        agg_type: string, which field to perform aggregation\n",
    "        join_original: boolean, True or False to join the original dataframe\n",
    "        calculate_group_diff_agg(df,['PX_LAST'], 'industry',True)\"\"\"\n",
    "        rst = defaultdict()\n",
    "        for agg_group in set([x[0] for x in self.df.columns]): \n",
    "            tkr_group = list(self.meta[self.meta[agg_type] == agg_group]['Ticker'].unique())\n",
    "            rst[agg_group]  = self.calculate_group_diff( flds,\n",
    "                            tkr_group, False  )\n",
    "        rst_df = pd.concat(rst.values(),axis=1)\n",
    "        if join_original == True: \n",
    "            return self.df.join(rst_df)\n",
    "        else:\n",
    "            return rst_df\n",
    "        \n",
    "    # calculate the difference between open interest call and put\n",
    "    def calculate_col_diff (self,col1, col2, col3): \n",
    "        \"\"\" this function creates difference between two columns in pandas multiindex dataframe-> col3 = col1 - col2\n",
    "        self.df: dataframe\n",
    "        col1, col2, col3: str, column name\n",
    "        calculate_col_diff (self.df,'OPEN_INT_TOTAL_CALL', 'OPEN_INT_TOTAL_PUT', 'OPEN_INT_DIFF')\n",
    "        \"\"\"\n",
    "        for i in set([x[0] for x in self.df.columns]) : \n",
    "            try: \n",
    "                self.df[i, col3] = self.df[i, col1] - self.df[i, col2 ]\n",
    "            except: \n",
    "                self.df[i, col3 ]  = np.nan\n",
    "        return self.df\n",
    "\n",
    "    def calculate_xd_chg(self, flds, days, join_original):\n",
    "        \"\"\"This function calculates x day changes for multiindex dataframe from pdblp\n",
    "        df:dataframe, input dataframe\n",
    "        flds:list, fields that you want to performe calculation on --[\"PX_LAST\", \"FUND_FLOW\",'VOLUME']\n",
    "        days:list,  x days of changes ,[5,15]\n",
    "        join_original: True or False, return calculated dataframe joined to the original df\n",
    "        eg: calculate_xd_chg(df, ['PX_LAST','VOLUME','FUND_FLOW','PX_HIGH'], [5,15,260],True)\"\"\"\n",
    "        rst = defaultdict()\n",
    "\n",
    "        for k_days in days:\n",
    "            subdf = self.df.loc[:,self.df.columns.get_level_values(1).isin(flds)]\n",
    "            rst[k_days] = subdf/subdf.shift(k_days)-1\n",
    "\n",
    "            i = subdf.columns.levels[1]   # OP's suggestion, for more flexibility!\n",
    "            j = [x + '_' + str(k_days) + '_dchg' for x in i ]\n",
    "            rename_dict = dict(zip(i, j))\n",
    "            rst[k_days].rename(columns=rename_dict, level=1,inplace = True)\n",
    "        rst_df = pd.concat(rst.values(),axis=1)\n",
    "        if join_original == True: \n",
    "            return self.df.join(rst_df)\n",
    "        else:\n",
    "            return rst_df\n",
    "    \n",
    "    def calculate_bollinger_band(self, flds, join_original):\n",
    "        \"\"\"This function calculates x day moving average for multiindex dataframe from pdblp\n",
    "        df:dataframe, input dataframe\n",
    "        flds:list, fields that you want to performe calculation on --[\"PX_LAST\", \"FUND_FLOW\",'VOLUME']\n",
    "        days:list,  x days of changes ,[5,15]\n",
    "        join_original: True or False, return calculated dataframe joined to the original df\n",
    "        eg: calculate_xd_ma(df, ['PX_LAST','VOLUME','FUND_FLOW','PX_HIGH'], [5,15,260],True)\"\"\"\n",
    "        rstdic =defaultdict( ) \n",
    "\n",
    "        subdf = self.df.loc[:,self.df.columns.get_level_values(1).isin(flds)]\n",
    "        rstdic['upper']=( subdf.rolling(20).mean() )  + 2 *( subdf.rolling(20).std())\n",
    "        rstdic['lower']=( subdf.rolling(20).mean() )  - 2 *( subdf.rolling(20).std())\n",
    "        for k in rstdic.keys(): \n",
    "            i = subdf.columns.levels[1]   # OP's suggestion, for more flexibility!\n",
    "            j = [x + '_bollinger_' + k for x in i ]\n",
    "            rename_dict = dict(zip(i, j))\n",
    "            rstdic[k].rename(columns=rename_dict, level=1,inplace = True)\n",
    "        rstdf = pd.concat(rstdic.values(), axis = 1)\n",
    "        if join_original == True: \n",
    "            return self.df.join(rstdf)\n",
    "        else:\n",
    "            return rstdf\n",
    "        \n",
    "    def calculate_xd_ma(self, flds, days, join_original):\n",
    "        \"\"\"This function calculates x day moving average for multiindex dataframe from pdblp\n",
    "        df:dataframe, input dataframe\n",
    "        flds:list, fields that you want to performe calculation on --[\"PX_LAST\", \"FUND_FLOW\",'VOLUME']\n",
    "        days:list,  x days of changes ,[5,15]\n",
    "        join_original: True or False, return calculated dataframe joined to the original df\n",
    "        eg: calculate_xd_ma(df, ['PX_LAST','VOLUME','FUND_FLOW','PX_HIGH'], [5,15,260],True)\"\"\"\n",
    "        rst = defaultdict()\n",
    "\n",
    "        for k_days in days:\n",
    "            subdf = self.df.loc[:,self.df.columns.get_level_values(1).isin(flds)]\n",
    "            rst[k_days] = subdf.rolling(k_days).mean()\n",
    "\n",
    "            i = subdf.columns.levels[1]   # OP's suggestion, for more flexibility!\n",
    "            j = [x + '_' + str(k_days) + '_dma' for x in i ]\n",
    "            rename_dict = dict(zip(i, j))\n",
    "            rst[k_days].rename(columns=rename_dict, level=1,inplace = True)\n",
    "        rst_df = pd.concat(rst.values(),axis=1)\n",
    "        if join_original == True: \n",
    "            return self.df.join(rst_df)\n",
    "        else:\n",
    "            return rst_df\n",
    "        \n",
    "    def calculate_xd_vol(self, flds, days, join_original):\n",
    "        \"\"\"This function calculates x day volatility for multiindex dataframe from pdblp\n",
    "        df:dataframe, input dataframe\n",
    "        flds:list, fields that you want to performe calculation on --[\"PX_LAST\", \"FUND_FLOW\"]\n",
    "        days:list,  x days of changes ,[5,15]\n",
    "        join_original: True or False, return calculated dataframe joined to the original df\n",
    "        eg: calculate_xd_vol(df, ['PX_LAST','FUND_FLOW'], [5,15,260],True)\"\"\"\n",
    "        rst = defaultdict()\n",
    "\n",
    "        for k_days in days:\n",
    "            subdf = self.df.loc[:,self.df.columns.get_level_values(1).isin(flds)]\n",
    "            rst[k_days] = subdf.rolling(k_days).std()\n",
    "\n",
    "            i = subdf.columns.levels[1]   # OP's suggestion, for more flexibility!\n",
    "            j = [x + '_' + str(k_days) + '_dstd' for x in i ]\n",
    "            rename_dict = dict(zip(i, j))\n",
    "            rst[k_days].rename(columns=rename_dict, level=1,inplace = True)\n",
    "        rst_df = pd.concat(rst.values(),axis=1)\n",
    "        if join_original == True: \n",
    "            return self.df.join(rst_df)\n",
    "        else:\n",
    "            return rst_df\n",
    "        \n",
    "    def calculate_xd_sharpe(self, flds, days, join_original):\n",
    "        \"\"\"This function calculates x day period sharpe for multiindex dataframe from pdblp\n",
    "        df:dataframe, input dataframe\n",
    "        flds:list, fields that you want to performe calculation on --[\"PX_LAST\"]\n",
    "        days:list,  x days of changes ,[5,15]\n",
    "        join_original: True or False, return calculated dataframe joined to the original df\n",
    "        eg: calculate_xd_sharpe(df, ['PX_LAST'], [5,15,260],True)\"\"\"\n",
    "        rst = defaultdict()\n",
    "        subdf = self.df.loc[:,self.df.columns.get_level_values(1).isin(flds)]\n",
    "        subdf_rts = subdf/ subdf.shift(1)-1\n",
    "        for k_days in days:\n",
    "            rst[k_days] =( np.sqrt(260)) * (subdf_rts.rolling(k_days).mean() / subdf_rts.rolling(k_days).std() )\n",
    "            i = subdf.columns.levels[1]   # OP's suggestion, for more flexibility!\n",
    "            j = [x + '_' + str(k_days) + '_dsharpe' for x in i ]\n",
    "            rename_dict = dict(zip(i, j))\n",
    "            rst[k_days].rename(columns=rename_dict, level=1,inplace = True)\n",
    "        rst_df = pd.concat(rst.values(),axis=1)\n",
    "        if join_original == True: \n",
    "            return self.df.join(rst_df)\n",
    "        else:\n",
    "            return rst_df\n",
    "        \n",
    "    def calculate_time_variable(self): \n",
    "        self.df['dayofweek'] = self.df.index.dayofweek\n",
    "        self.df['weekofmonth'] = list(pd.Series(self.df.index).apply(lambda d: (d.day-1) // 7 + 1))\n",
    "        self.df['weekofyear'] = self.df.index.weekofyear\n",
    "        return self.df\n",
    "\n",
    "    def convert_categorical(self, categorical_cols): \n",
    "        \"\"\"this function converts categorical columns into numeric columns before classification\n",
    "        self.df : dataframe\n",
    "        categorical_cols: list of categorical columns to convert to\"\"\"\n",
    "        for i in categorical_cols : \n",
    "            self.df[i] = pd.factorize( self.df[i] )[0]\n",
    "        return self.df\n",
    "\n",
    "    def generate_final_df(self, varlist): \n",
    "        subdf = self.df.loc[:,self.df.columns.get_level_values(1).isin(varlist)]\n",
    "        finaldf = pd.DataFrame()\n",
    "        for i in list(set([x[0] for x in subdf.columns])) : \n",
    "            subdf[i, 'ticker'] = i\n",
    "            subdf [i, 'industry'] = industrydic[i]\n",
    "            finaldf = finaldf.append(subdf[i])\n",
    "\n",
    "        self.df = finaldf.apply(lambda x: x.replace( [np.inf, -np.inf], np.nan))\n",
    "        self.df = self.calculate_time_variable()\n",
    "        self.df = self.convert_categorical( ['industry'])\n",
    "        self.df = self.df.sort_index()\n",
    "        \n",
    "        return self.df\n",
    "\n",
    "\n",
    "    def convert_y_label(self, label): \n",
    "        \"\"\"label->list\"\"\"\n",
    "        for i in label : \n",
    "            self.df[i] = np.where(self.df[i]<0, -1 ,self.df[i])\n",
    "            self.df[i] = np.where(self.df[i]>0, 1 ,self.df[i])\n",
    "        return self.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class feature_selection() :\n",
    "    def __init__(self, input_df):\n",
    "        self.df = input_df\n",
    "        \n",
    "    def get_rf_feature_importance(self, varlist, y):\n",
    "        \"\"\"This function returns random forest feature importance data in a dataframe\n",
    "        varlist: list, a list of features to put into random forest\n",
    "        y: target variable\n",
    "        df: input dataframe\n",
    "        get_rf_feature_importance ( ['cap','OPEN_INT_DIFF','FUND_FLOW_5_dchg','VOLUME_5_dchg','VOLUME_260_dchg'],\n",
    "        'y_15_d',finaldf)\"\"\"\n",
    "        inputdf = (self.df[varlist + [y] ].dropna())\n",
    "        cutoff = int( 0.8 * len(inputdf))\n",
    "        xtrain,ytrain = inputdf.iloc[:cutoff][varlist], inputdf.iloc[:cutoff][[y]]\n",
    "        xtest,ytest =  inputdf.iloc[cutoff:][varlist], inputdf.iloc[cutoff:][[y]]\n",
    "\n",
    "        rf = RandomForestClassifier(n_estimators= 500,n_jobs=1 ).fit(xtrain, ytrain)\n",
    "        y_pred = rf.predict(xtest)\n",
    "        testdf =  inputdf.iloc[cutoff:][varlist + [y]]\n",
    "        testdf['y_pred'] = y_pred\n",
    "        feature_importances = pd.DataFrame(rf.feature_importances_,\n",
    "                                       index = xtrain.columns,\n",
    "                                        columns=['importance']).sort_values('importance',  ascending=False)\n",
    "        return feature_importances\n",
    "\n",
    "        \n",
    "    def get_precision_feature_importance(self, varlist, y): \n",
    "        \"\"\"This function pass in features one by one from the varlist to build random forest and compare precision score to select \n",
    "        the best feature\"\"\"\n",
    "\n",
    "        precision_df = pd.DataFrame(index = varlist, columns= ['precision'])\n",
    "        precision_df.index.name = 'field'\n",
    "\n",
    "        for x in varlist: \n",
    "            inputdf =  (self.df[  [x,y] ].dropna())\n",
    "            cutoff = int( 0.8 * len(inputdf))\n",
    "            xtrain,ytrain = inputdf.iloc[:cutoff][[x]], inputdf.iloc[:cutoff][[y]]\n",
    "            xtest,ytest =  inputdf.iloc[cutoff:][[x]], inputdf.iloc[cutoff:][[y]]\n",
    "\n",
    "            rf = RandomForestClassifier(n_estimators= 500).fit(xtrain, ytrain)\n",
    "            y_pred = rf.predict(xtest)\n",
    "            precision_df.loc[x] = precision_score(ytest, y_pred, average = 'weighted')\n",
    "        precision_df = precision_df.sort_values(by  ='precision', ascending = False, )\n",
    "        return precision_df\n",
    "    \n",
    "    def select_features(self, varlist, y, method): \n",
    "        \"\"\"This function ranks feature by importance using the get_rf_feature_importance and  get_precision_feature_importance\n",
    "        function\n",
    "        varlist : list \n",
    "        y: str\n",
    "        df: dataframe\n",
    "        method: str  \"\"\"\n",
    "        if method == 'feature_importance' : \n",
    "            feature_importance = self.get_rf_feature_importance ( varlist, y)\n",
    "            return feature_importance\n",
    "        if method == 'precision_importance':\n",
    "            precision_importance = self.get_precision_feature_importance( varlist, y).sort_values(by = 'precision', ascending = False)\n",
    "            return precision_importance\n",
    "        if method =='both':\n",
    "            feature_importance = self.get_rf_feature_importance ( varlist, y)\n",
    "            precision_importance =  self.get_precision_feature_importance( varlist, y).sort_values(by = 'precision', ascending = False)\n",
    "            total = precision_importance.join(feature_importance)\n",
    "            total['total'] = total.sum(axis =1 )\n",
    "            total = total.sort_values(by = 'total', ascending = False)\n",
    "            return total\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model_building(): \n",
    "    def __init__(self, df): \n",
    "        self.df = df\n",
    "        \n",
    "    def build_models(self, feature_list, y, model_param ): \n",
    "        \"\"\"This function build all classification models for model selection purpose\n",
    "        it returns, model, prediction and confusion matrix\n",
    "        feature_list: list, a list of features to feed into the dataframe\n",
    "        y: string, for y variable\n",
    "        df: input dataframe\n",
    "        build_models(feature_ranking , 4, 'y_15_d', finaldf.tail(400))\"\"\"\n",
    "\n",
    "        inputdf = self.df[feature_list + [y,'ticker']].dropna().sort_index()\n",
    "        print(len(inputdf))\n",
    "        print(feature_list)\n",
    "        modeldic, modelpred = {},{}\n",
    "        scores = pd.DataFrame()\n",
    "        cutoff = int( 0.8 * len(inputdf))\n",
    "        xtrain,ytrain = inputdf.iloc[:cutoff][feature_list], inputdf.iloc[:cutoff][[y]]\n",
    "        xtest,ytest =  inputdf.iloc[cutoff:][feature_list], inputdf.iloc[cutoff:][[y]]\n",
    "\n",
    "        for i , j in model_param.items():\n",
    "            modeldic[i] = j.fit(xtrain, ytrain)\n",
    "            testdf =  inputdf.iloc[cutoff:][  [y,'ticker']]\n",
    "            testdf['y_pred'] = modeldic[i].predict(xtest)\n",
    "            try:\n",
    "                testdf[['p(-1)','p(0)','p(1)']] = modeldic[i].predict_proba(xtest)\n",
    "            except:\n",
    "                None\n",
    "            modelpred[i]  = testdf\n",
    "            scores.loc[i,'accuracy'] = accuracy_score (ytest,modelpred[i]['y_pred']  )\n",
    "            scores.loc[i,'precision'] = precision_score (ytest,modelpred[i]['y_pred'] , average = 'weighted')\n",
    "            scores.loc[i,'recall'] = recall_score (ytest,modelpred[i]['y_pred']  , average = 'weighted' )\n",
    "            scores.loc[i,'f1'] = f1_score (ytest,modelpred[i]['y_pred']  , average = 'weighted')\n",
    "        return modeldic , modelpred , scores\n",
    "\n",
    "    \n",
    "    ## full backtest model\n",
    "    def build_models_full(self, feature_list, y, model_param ): \n",
    "        modeldic, modelpred = {},{}\n",
    "        inputdf = self.df[feature_list + [y,'ticker']].dropna().sort_index()\n",
    "        print(len(inputdf))\n",
    "        print(feature_list)\n",
    "        modeldic, modelpred = {},{}\n",
    "        scores = pd.DataFrame()\n",
    "\n",
    "        for i in model_param.keys(): \n",
    "            modelpred[i] = pd.DataFrame()\n",
    "\n",
    "        for idx  in range(0,10,2):\n",
    "            # specify start and end interval for test set\n",
    "            int_start , int_end = int( (idx/10) * len(inputdf)), int( (idx/10 + 0.2) * len(inputdf))\n",
    "            xtest,ytest =  inputdf.iloc[int_start:int_end][feature_list], inputdf.iloc[int_start:int_end][[y]]\n",
    "            xtrain, ytrain = inputdf[~inputdf.index.isin(xtest.index)][feature_list] ,inputdf[~inputdf.index.isin(ytest.index)][[y]]\n",
    "\n",
    "            for i , j in model_param.items():\n",
    "                modeldic[i] = j.fit(xtrain, ytrain)\n",
    "                testdf =  inputdf.iloc[int_start:int_end][ [y,'ticker']]\n",
    "                testdf['y_pred'] = modeldic[i].predict(xtest)\n",
    "                try:\n",
    "                    testdf[['p(-1)','p(0)','p(1)']] = modeldic[i].predict_proba(xtest)\n",
    "                except:\n",
    "                    None\n",
    "                modelpred[i] = modelpred[i].append( testdf ) \n",
    "\n",
    "        for i in model_param.keys():\n",
    "            scores.loc[i,'accuracy'] = accuracy_score (inputdf[[y]],modelpred[i]['y_pred']  )\n",
    "            scores.loc[i,'precision'] = precision_score (inputdf[[y]],modelpred[i]['y_pred'] , average = 'weighted')\n",
    "            scores.loc[i,'recall'] = recall_score (inputdf[[y]],modelpred[i]['y_pred']  , average = 'weighted' )\n",
    "            scores.loc[i,'f1'] = f1_score (inputdf[[y]],modelpred[i]['y_pred']  , average = 'weighted')\n",
    "        return modeldic , modelpred , scores\n",
    "    \n",
    "    def build_final_model(self, feature_list, y, model_param): \n",
    "        modeldic, modelpred = {},{}\n",
    "        inputdf = self.df[feature_list + [y,'ticker']].dropna().sort_index()\n",
    "        X_df, y_df = inputdf[feature_list], inputdf[[y]]\n",
    "        for i , j in model_param.items():\n",
    "            modeldic[i] = j.fit(X_df, y_df)\n",
    "        return modeldic\n",
    "    \n",
    "    def get_latest_prediction(self, model,df, sort_order) :  \n",
    "\n",
    "        df =  df.loc['2021-01-01':].reset_index().set_index(['ticker','date'])\n",
    "        df = df.dropna()\n",
    "        df['y_pred'] =model.predict( df)\n",
    "        final_pred = df[['y_pred']].reset_index()\n",
    "        final_pred = final_pred[ final_pred.date==list(final_pred.date)[-1]]\n",
    "        sort_order.index.name = 'ticker'\n",
    "        final_pred = final_pred.merge(sort_order.reset_index(), how = 'outer').sort_values(by = 'correct', ascending = False)\n",
    "        final_pred = final_pred[final_pred['y_pred']!=0].set_index('date')\n",
    "        return final_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class backtest(): \n",
    "    def portfolio_stats_matrix(self, daily_rts, input_signal, strategy, weights_type):\n",
    "        \"\"\"inputsignal :is a weekly dataframe\n",
    "        This function generates backtest result and stats, give asset daily returns, input signal/weights\n",
    "        daily_rts: dataframe of daily returns of assets \n",
    "        input_signal: dataframe of asset weights/sigmal\n",
    "        strategy: str -- 'long only' to be expanded to 'long short'\n",
    "        weights_type: str, equal or weighted, to decide if the signal weights\"\"\"\n",
    "        rf = 0.02\n",
    "        input_weights = input_signal.reset_index().set_index(['date','ticker'])[['y_pred']].unstack()\n",
    "        input_weights.columns  = input_weights.columns.droplevel(0)\n",
    "        input_weights = input_weights.resample('D').first().fillna(method = 'ffill')\n",
    "        if strategy == 'long only':\n",
    "            input_weights[input_weights<0] = 0\n",
    "        if weights_type =='weighted': \n",
    "            input_weights = input_weights.div(input_weights.sum(axis=1), axis=0)\n",
    "        elif weights_type =='equal': \n",
    "            input_weights[input_weights>0]=1\n",
    "            input_weights= input_weights.div(input_weights.sum(axis=1), axis=0)\n",
    "\n",
    "        #input_weights = input_weights [input_weights!= 0].dropna(how='all',axis=1).fillna(0).sort_index()\n",
    "\n",
    "        returns = daily_rts[input_weights.columns].loc[input_weights.index[0]: input_weights.index[-1]]\n",
    "        returns['portfolio_returns'] = (returns*input_weights).sum(axis = 1)\n",
    "        returns['portfolio_cum_returns'] = (returns['portfolio_returns']+1).cumprod() -1\n",
    "\n",
    "        port_mean_returns = returns['portfolio_returns'].mean()*260\n",
    "        port_total_returns =  returns['portfolio_cum_returns'][-1]\n",
    "        port_sigma = returns['portfolio_returns'].std() * np.sqrt(260)\n",
    "        port_sharpe = (port_mean_returns - rf)/ port_sigma\n",
    "        return round(port_mean_returns,2), round(port_sigma,2),round(port_sharpe,2),round(port_total_returns,2), returns[['portfolio_cum_returns']]\n",
    "\n",
    "\n",
    "    def calculate_model_score(self, modelpred):\n",
    "        \"\"\"This function calculates the correction score for each model in the model pred dictioanry\"\"\"\n",
    "        rst = pd.DataFrame()\n",
    "        for i in modelpred.keys(): \n",
    "            label = [x for x in modelpred[i].keys() if x !='ticker' and x!= 'y_pred'][0]\n",
    "            subdf = modelpred[i][modelpred[i][label]!=0]\n",
    "            rst.loc['correct',i] = len(subdf[subdf[label] == subdf['y_pred']]) / len(subdf)\n",
    "            rst.loc['wrong',i] = len(subdf[(subdf[label] != subdf['y_pred']) & (subdf['y_pred']!=0)])/ len(subdf)\n",
    "            rst.loc['miss',i] =  (len(subdf[(subdf[label] != subdf['y_pred']) & (subdf['y_pred']==0)])/ len(subdf))\n",
    "        return rst\n",
    "\n",
    "    def calculate_fund_score(self, modelpred): \n",
    "        rst = {}\n",
    "        label = [x for x in modelpred['gb'].keys() if '_d' in x][0]\n",
    "        for i in modelpred.keys():\n",
    "            rst[i] = pd.DataFrame()\n",
    "            for tkr in modelpred[i]['ticker'].unique(): \n",
    "                subdf = modelpred[i] [modelpred[i]['ticker'] == tkr ]\n",
    "\n",
    "                rst[i].loc[tkr, 'correct'] = len(subdf[subdf[label] == subdf['y_pred']]) / len(subdf)\n",
    "                rst[i].loc[tkr, 'wrong'] = len(subdf[(subdf[label] != subdf['y_pred']) & (subdf['y_pred']!=0)])/ len(subdf)\n",
    "                rst[i].loc[tkr, 'miss'] =  (len(subdf[(subdf[label] != subdf['y_pred']) & (subdf['y_pred']==0)])/ len(subdf))\n",
    "                rst[i].loc[tkr, 'predicted_n'] = abs(subdf['y_pred']).sum()\n",
    "                rst[i] = rst[i][rst[i]['predicted_n']!=0]\n",
    "                rst[i].sort_values(by ='correct',ascending = False, inplace = True)\n",
    "        return rst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get final result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fe = feature_engineering()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fe.df = fe.calculate_y_label( 15, 0.075, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fe.df =  fe.calculate_col_diff ('OPEN_INT_TOTAL_CALL', 'OPEN_INT_TOTAL_PUT', 'OPEN_INT_DIFF')\n",
    "fe.df =  fe.calculate_col_diff ('PX_BID', 'PX_ASK', 'BID_ASK_DIFF')\n",
    "fe.df = fe.calculate_xd_ma( ['PX_LAST','VOLUME'], [5,90,260],True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 10.5 GiB for an array with shape (18068, 78306) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-4224f38c5af3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mfe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalculate_xd_sharpe\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'PX_LAST'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m90\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m260\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mfe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalculate_structural_break\u001b[0m \u001b[1;33m(\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'VOLUME'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'PX_LAST'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'OPEN_INT_DIFF'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mfe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalculate_group_diff_agg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'PX_LAST'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'VOLUME'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'industry'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-5-d5efc1a9c964>\u001b[0m in \u001b[0;36mcalculate_group_diff_agg\u001b[1;34m(self, flds, agg_type, join_original)\u001b[0m\n\u001b[0;32m     95\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0magg_group\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m             \u001b[0mtkr_group\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmeta\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmeta\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0magg_type\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0magg_group\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Ticker'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 97\u001b[1;33m             rst[agg_group]  = self.calculate_group_diff( flds,\n\u001b[0m\u001b[0;32m     98\u001b[0m                             tkr_group, False  )\n\u001b[0;32m     99\u001b[0m         \u001b[0mrst_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-d5efc1a9c964>\u001b[0m in \u001b[0;36mcalculate_group_diff\u001b[1;34m(self, flds, tkr_group, join_original)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi_flds\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mflds\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m             \u001b[0msubdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtkr_group\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtkr_group\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi_flds\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m             \u001b[0mrst\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi_flds\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m             \u001b[0mi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlevels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m   \u001b[1;31m# OP's suggestion, for more flexibility!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2912\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2914\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_take_with_is_copy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2915\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2916\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_single_key\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_take_with_is_copy\u001b[1;34m(self, indices, axis)\u001b[0m\n\u001b[0;32m   3361\u001b[0m         \u001b[0mSee\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdocstring\u001b[0m \u001b[0mof\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfull\u001b[0m \u001b[0mexplanation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3362\u001b[0m         \"\"\"\n\u001b[1;32m-> 3363\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3364\u001b[0m         \u001b[1;31m# Maybe set copy if we didn't actually change the index.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3365\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mequals\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mtake\u001b[1;34m(self, indices, axis, is_copy, **kwargs)\u001b[0m\n\u001b[0;32m   3346\u001b[0m         \u001b[0mnv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalidate_take\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3347\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3348\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3349\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3350\u001b[0m         new_data = self._mgr.take(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_consolidate_inplace\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   5212\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mgr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconsolidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5213\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5214\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_protect_consolidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5215\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5216\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_consolidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool_t\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_protect_consolidate\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m   5201\u001b[0m         \"\"\"\n\u001b[0;32m   5202\u001b[0m         \u001b[0mblocks_before\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5203\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mblocks_before\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5205\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_clear_item_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mf\u001b[1;34m()\u001b[0m\n\u001b[0;32m   5210\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5211\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5212\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mgr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconsolidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5213\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5214\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_protect_consolidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mconsolidate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    972\u001b[0m         \u001b[0mbm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    973\u001b[0m         \u001b[0mbm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_consolidated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 974\u001b[1;33m         \u001b[0mbm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    975\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mbm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    976\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36m_consolidate_inplace\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    977\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_consolidate_inplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    978\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_consolidated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 979\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_consolidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    980\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_consolidated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    981\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_known_consolidated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36m_consolidate\u001b[1;34m(blocks)\u001b[0m\n\u001b[0;32m   1897\u001b[0m     \u001b[0mnew_blocks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1898\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0m_can_consolidate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup_blocks\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgrouper\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1899\u001b[1;33m         merged_blocks = _merge_blocks(\n\u001b[0m\u001b[0;32m   1900\u001b[0m             \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup_blocks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcan_consolidate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_can_consolidate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1901\u001b[0m         )\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36m_merge_blocks\u001b[1;34m(blocks, dtype, can_consolidate)\u001b[0m\n\u001b[0;32m   1920\u001b[0m         \u001b[1;31m# combination of those slices is a slice, too.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1921\u001b[0m         \u001b[0mnew_mgr_locs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmgr_locs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_array\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mblocks\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1922\u001b[1;33m         \u001b[0mnew_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mblocks\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1923\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1924\u001b[0m         \u001b[0margsort\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_mgr_locs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mvstack\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\core\\shape_base.py\u001b[0m in \u001b[0;36mvstack\u001b[1;34m(tup)\u001b[0m\n\u001b[0;32m    281\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m         \u001b[0marrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0marrs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 283\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 10.5 GiB for an array with shape (18068, 78306) and data type float64"
     ]
    }
   ],
   "source": [
    "fe.df = fe.calculate_xd_sharpe( ['PX_LAST'], [5,90,260],True)\n",
    "fe.df = fe.calculate_structural_break ( ['VOLUME','PX_LAST','OPEN_INT_DIFF'], True)\n",
    "fe.df = fe.calculate_group_diff_agg(['PX_LAST','VOLUME'], 'industry',True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fe.df = fe.calculate_bollinger_band( ['PX_LAST'], True)    \n",
    "fe.df = fe.calculate_col_diff ('FUND_FLOW_90_dma', 'FUND_FLOW_5_dma', 'FUND_FLOW_90_5_DIFF')\n",
    "fe.df = fe.calculate_col_diff ('PX_LAST_260_dma', 'PX_LAST_90_dma', 'PX_LAST_260_90_DIFF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fe.df = fe.df.resample('W').last()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varlist = ['BID_ASK_DIFF','PX_LAST_5_dsharpe','PX_LAST_90_dsharpe','PX_LAST_260_dsharpe','PX_LAST_sb','PX_LAST_bollinger_upper',\n",
    "          'PE_RATIO','OPEN_INT_DIFF']\n",
    "y = 'y_15_d'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fe.df = fe.generate_final_df(varlist + [y])\n",
    "fe.df = fe.convert_y_label([y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get final prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalmodels = pd.read_pickle( 'finalmodels.pkl')\n",
    "rst = pd.read_pickle('rst.pkl')\n",
    "\n",
    "for i in rst.keys(): \n",
    "    rst[i]['predicted_correct_n'] = rst[i]['correct'] * rst[i]['predicted_n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mb = model_building( fe.df)\n",
    "latest_rst  = mb.get_latest_prediction(finalmodels['gb'], fe.df[input_features+ ['ticker']],  rst['gb'][['correct','predicted_n']] )\n",
    "past_rst =  mb.get_past_prediction(finalmodels['gb'], fe.df[input_features+ ['ticker']],  rst['gb'][['correct','predicted_n']] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bt = backtest()\n",
    "backtest =bt.portfolio_stats_matrix(fe.daily_rts, past_rst,'long only','weighted')\n",
    "backtest[-1].plot(title = 'Backtest sharpe ' + i + ':' + str(backtest[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_rst.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
